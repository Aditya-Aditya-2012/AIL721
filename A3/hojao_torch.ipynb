{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e162c1b2-9e5a-40c7-bdd7-ecb92f785af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import argparse\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Try to import torchtext. If unavailable, we'll fall back to our own tokenizer.\n",
    "try:\n",
    "    import torchtext\n",
    "    from torchtext.data.utils import get_tokenizer\n",
    "    from torchtext.vocab import build_vocab_from_iterator\n",
    "    torchtext_available = True\n",
    "except ImportError:\n",
    "    torchtext_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2070b785-89f0-48fb-b8cf-ef81cc675384",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600       # Maximum sequence length\n",
    "max_tokens = 20000     # Maximum vocabulary size\n",
    "batch_size = 32\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "num_classes = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52647eb4-f3f1-457c-ade3-20b27720f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset that reads text files from a directory structure.\n",
    "    Expects subdirectories for each category containing .txt files.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, vocab, tokenizer, max_length=600):\n",
    "        self.samples = []\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.label2idx = {}\n",
    "        self.idx2label = {}\n",
    "        # Get categories sorted alphabetically\n",
    "        categories = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
    "        for i, cat in enumerate(categories):\n",
    "            self.label2idx[cat] = i\n",
    "            self.idx2label[i] = cat\n",
    "            cat_dir = os.path.join(root_dir, cat)\n",
    "            files = glob.glob(os.path.join(cat_dir, \"*.txt\"))\n",
    "            for f in files:\n",
    "                with open(f, \"r\", encoding=\"utf-8\") as fp:\n",
    "                    text = fp.read().strip()\n",
    "                self.samples.append((text, i))\n",
    "        random.shuffle(self.samples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.samples[idx]\n",
    "        tokens = self.tokenizer(text)\n",
    "        # Convert tokens to indices; if token not found, use index 0 (<unk>)\n",
    "        indices = [self.vocab.get(token, 0) for token in tokens]\n",
    "        # Pad or truncate the sequence to max_length\n",
    "        if len(indices) < self.max_length:\n",
    "            indices = indices + [0] * (self.max_length - len(indices))\n",
    "        else:\n",
    "            indices = indices[:self.max_length]\n",
    "        return torch.tensor(indices, dtype=torch.long), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "377755c1-2eb7-4a31-99b6-25ad910c3ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(data_dir, tokenizer, use_torchtext=True, max_tokens=20000):\n",
    "    \"\"\"\n",
    "    Build a vocabulary from text files located in data_dir (assumed to be the training directory).\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    categories = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "    for cat in categories:\n",
    "        cat_dir = os.path.join(data_dir, cat)\n",
    "        files = glob.glob(os.path.join(cat_dir, \"*.txt\"))\n",
    "        for f in files:\n",
    "            with open(f, \"r\", encoding=\"utf-8\") as fp:\n",
    "                text = fp.read().strip()\n",
    "            texts.append(text)\n",
    "    \n",
    "    def yield_tokens(texts):\n",
    "        for t in texts:\n",
    "            yield tokenizer(t)\n",
    "    \n",
    "    if use_torchtext and torchtext_available:\n",
    "        # Use torchtext's vocabulary builder\n",
    "        vocab = build_vocab_from_iterator(yield_tokens(texts), specials=[\"<unk>\"], max_tokens=max_tokens)\n",
    "        vocab.set_default_index(vocab[\"<unk>\"])\n",
    "        # Convert torchtext Vocab to a regular dict mapping token -> index\n",
    "        vocab_dict = {token: vocab[token] for token in vocab.get_itos()}\n",
    "        return vocab_dict\n",
    "    else:\n",
    "        # Manually build vocabulary using collections.Counter\n",
    "        counter = Counter()\n",
    "        for t in texts:\n",
    "            tokens = tokenizer(t)\n",
    "            counter.update(tokens)\n",
    "        most_common = counter.most_common(max_tokens - 1)  # reserve index 0 for <unk>\n",
    "        vocab = {\"<unk>\": 0}\n",
    "        for idx, (token, count) in enumerate(most_common, start=1):\n",
    "            vocab[token] = idx\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b847d1e-9c4a-407a-ba11-0d3b9f05d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds token embeddings and learned positional embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = nn.Embedding(sequence_length, embed_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        batch_size, seq_length = x.size()\n",
    "        positions = torch.arange(0, seq_length, device=x.device).unsqueeze(0).expand(batch_size, seq_length)\n",
    "        token_emb = self.token_embeddings(x)          # (batch_size, seq_length, embed_dim)\n",
    "        pos_emb = self.position_embeddings(positions)   # (batch_size, seq_length, embed_dim)\n",
    "        return token_emb + pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95f5f1f4-f8f4-4086-9ac0-307264744705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer encoder block with multi-head self-attention, a feedforward network, \n",
    "    and residual connections with layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.linear1 = nn.Linear(embed_dim, dense_dim)\n",
    "        self.linear2 = nn.Linear(dense_dim, embed_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        # x: (batch_size, seq_length, embed_dim)\n",
    "        attn_output, _ = self.multihead_attn(x, x, x, attn_mask=src_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.linear2(self.activation(self.linear1(x)))\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4d74910-e4d1-4259-bbcb-dfc39da02aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete model: embeddings, transformer encoder, global max pooling, dropout, and a classification head.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, num_heads, dense_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = PositionalEmbedding(sequence_length, vocab_size, embed_dim)\n",
    "        self.transformer = TransformerEncoder(embed_dim, dense_dim, num_heads)\n",
    "        # Global max pooling is implemented using adaptive max pooling over the time dimension.\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_length)\n",
    "        x = self.embedding(x)            # (batch_size, seq_length, embed_dim)\n",
    "        x = self.transformer(x)          # (batch_size, seq_length, embed_dim)\n",
    "        # Permute to (batch_size, embed_dim, seq_length) for pooling\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pool(x).squeeze(2)      # (batch_size, embed_dim)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46945a52-e226-4de0-af6d-257083b0fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# Training and Evaluation\n",
    "# ------------------------\n",
    "def train_model(model, train_loader, val_loader, device, epochs=5, lr=1e-3):\n",
    "    train_accuracy_array = []\n",
    "    val_accuracy_array = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_corrects += torch.sum(preds == labels).item()\n",
    "            total += inputs.size(0)\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = running_corrects / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "        train_accuracy_array.append(epoch_acc)\n",
    "        # Validate\n",
    "        model.eval()\n",
    "        val_corrects = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_corrects += torch.sum(preds == labels).item()\n",
    "                val_total += inputs.size(0)\n",
    "        val_acc = val_corrects / val_total\n",
    "        val_accuracy_array.append(val_acc)\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "    print(\"Training complete. Best validation accuracy: {:.4f}\".format(best_val_acc))\n",
    "    return train_accuracy_array, val_accuracy_array\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_corrects = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            test_corrects += torch.sum(preds == labels).item()\n",
    "            test_total += inputs.size(0)\n",
    "    test_acc = test_corrects / test_total\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad8e18ba-1753-47d1-866f-2376ed65d991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torchtext tokenizer (basic_english)\n",
      "Vocabulary size: 20000\n",
      "TextClassificationModel(\n",
      "  (embedding): PositionalEmbedding(\n",
      "    (token_embeddings): Embedding(20000, 256)\n",
      "    (position_embeddings): Embedding(600, 256)\n",
      "  )\n",
      "  (transformer): TransformerEncoder(\n",
      "    (multihead_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=256, out_features=32, bias=True)\n",
      "    (linear2): Linear(in_features=32, out_features=256, bias=True)\n",
      "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (activation): ReLU()\n",
      "  )\n",
      "  (pool): AdaptiveMaxPool1d(output_size=1)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      ")\n",
      "Epoch 1/20 - Train Loss: 2.4921 Acc: 0.2261\n",
      "Validation Accuracy: 0.3818\n",
      "Epoch 2/20 - Train Loss: 1.5657 Acc: 0.3233\n",
      "Validation Accuracy: 0.3750\n",
      "Epoch 3/20 - Train Loss: 1.3908 Acc: 0.4104\n",
      "Validation Accuracy: 0.4426\n",
      "Epoch 4/20 - Train Loss: 1.1370 Acc: 0.5494\n",
      "Validation Accuracy: 0.5574\n",
      "Epoch 5/20 - Train Loss: 0.8328 Acc: 0.6951\n",
      "Validation Accuracy: 0.6486\n",
      "Epoch 6/20 - Train Loss: 0.6697 Acc: 0.7630\n",
      "Validation Accuracy: 0.6284\n",
      "Epoch 7/20 - Train Loss: 0.4889 Acc: 0.8317\n",
      "Validation Accuracy: 0.6993\n",
      "Epoch 8/20 - Train Loss: 0.3661 Acc: 0.8786\n",
      "Validation Accuracy: 0.7095\n",
      "Epoch 9/20 - Train Loss: 0.2371 Acc: 0.9372\n",
      "Validation Accuracy: 0.7162\n",
      "Epoch 10/20 - Train Loss: 0.1619 Acc: 0.9573\n",
      "Validation Accuracy: 0.7466\n",
      "Epoch 11/20 - Train Loss: 0.1227 Acc: 0.9690\n",
      "Validation Accuracy: 0.7230\n",
      "Epoch 12/20 - Train Loss: 0.1223 Acc: 0.9707\n",
      "Validation Accuracy: 0.7500\n",
      "Epoch 13/20 - Train Loss: 0.1039 Acc: 0.9690\n",
      "Validation Accuracy: 0.7736\n",
      "Epoch 14/20 - Train Loss: 0.0951 Acc: 0.9724\n",
      "Validation Accuracy: 0.7264\n",
      "Epoch 15/20 - Train Loss: 0.0396 Acc: 0.9908\n",
      "Validation Accuracy: 0.7635\n",
      "Epoch 16/20 - Train Loss: 0.0255 Acc: 0.9950\n",
      "Validation Accuracy: 0.7466\n",
      "Epoch 17/20 - Train Loss: 0.0096 Acc: 1.0000\n",
      "Validation Accuracy: 0.7432\n",
      "Epoch 18/20 - Train Loss: 0.0079 Acc: 1.0000\n",
      "Validation Accuracy: 0.7500\n",
      "Epoch 19/20 - Train Loss: 0.0090 Acc: 0.9975\n",
      "Validation Accuracy: 0.7568\n",
      "Epoch 20/20 - Train Loss: 0.1199 Acc: 0.9682\n",
      "Validation Accuracy: 0.6655\n",
      "Training complete. Best validation accuracy: 0.7736\n",
      "Test Accuracy: 0.7456\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------\n",
    "# Main Function\n",
    "# ------------------------\n",
    "\n",
    "use_torchtext = True\n",
    "if use_torchtext and torchtext_available:\n",
    "    print(\"Using torchtext tokenizer (basic_english)\")\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "else:\n",
    "    print(\"Using basic Python tokenizer (split on whitespace)\")\n",
    "    tokenizer = lambda x: x.lower().split()\n",
    "\n",
    "train_dir = os.path.join(\"Datasets\", \"train\")\n",
    "vocab = build_vocab(train_dir, tokenizer, use_torchtext=use_torchtext, max_tokens=max_tokens)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TextClassificationDataset(train_dir, vocab, tokenizer, max_length)\n",
    "val_dir = os.path.join(\"Datasets\", \"val\")\n",
    "val_dataset = TextClassificationDataset(val_dir, vocab, tokenizer, max_length)\n",
    "test_dir = os.path.join(\"Datasets\", \"test\")\n",
    "test_dataset = TextClassificationDataset(test_dir, vocab, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model\n",
    "model = TextClassificationModel(sequence_length=max_length,\n",
    "                                vocab_size=vocab_size,\n",
    "                                embed_dim=embed_dim,\n",
    "                                num_heads=num_heads,\n",
    "                                dense_dim=dense_dim,\n",
    "                                num_classes=num_classes)\n",
    "print(model)\n",
    "\n",
    "# Train and validate the model\n",
    "nepochs=20\n",
    "train_array, val_array = train_model(model, train_loader, val_loader, device, epochs=nepochs)\n",
    "\n",
    "# Load the best model and evaluate on the test set\n",
    "model.load_state_dict(torch.load(\"best_model.pth\", map_location=device))\n",
    "test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31acbe3d-ab90-4404-a12c-02689c4bda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume train_array and val_array are lists or numpy arrays of the same length.\n",
    "epochs = range(1, len(train_array) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_array, 'b-o', label='Train Metric')\n",
    "plt.plot(epochs, val_array, 'r-o', label='Validation Metric')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')  # Change label as needed (e.g., \"Loss\" or \"Accuracy\")\n",
    "plt.title('Training and Validation Metrics using basic tokenizer')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot to a file\n",
    "plt.savefig('accuracy_plot_simple.png')\n",
    "\n",
    "# Optionally, display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f92322-52ef-438d-8112-7aa96bddd056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy using basic = 77.41%\n",
    "# test accuracy using torchtext = 74.42%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6e02c-b1e9-467d-b191-805b18c113b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIL721",
   "language": "python",
   "name": "ail721"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
