{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7945afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "dataset_root = \"../Datasets\"\n",
    "#maxLen = 18387\n",
    "#vocab_size = 24787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75daf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/TrainData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb61bade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_data(sent):\n",
    "    sent = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", sent)\n",
    "    sent = re.sub(r\"\\'s\", \" \\'s\", sent)\n",
    "    sent = re.sub(r\"\\'ve\", \" \\'ve\", sent)\n",
    "    sent = re.sub(r\"n\\'t\", \" n\\'t\", sent)\n",
    "    sent = re.sub(r\"\\'re\", \" \\'re\", sent)\n",
    "    sent = re.sub(r\"\\'d\", \" \\'d\", sent)\n",
    "    sent = re.sub(r\"\\'ll\", \" \\'ll\", sent)\n",
    "    sent = re.sub(r\",\", \" , \", sent)\n",
    "    sent = re.sub(r\"!\", \" ! \", sent)\n",
    "    sent = re.sub(r\"\\(\", \" \\( \", sent)\n",
    "    sent = re.sub(r\"\\)\", \" \\) \", sent)\n",
    "    sent = re.sub(r\"\\?\", \" \\? \", sent)\n",
    "    sent = re.sub(r\"\\s{2,}\", \" \", sent)\n",
    "    return sent\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = _clean_data(text)\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6219bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['Text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f630c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 1\n",
    "\n",
    "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "word_counts = Counter(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c15f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: i for i, (word, count) in enumerate(word_counts.items()) if count >= min_freq}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23224635",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.array([word_counts[word] for word in vocab.keys()], dtype=np.float32)\n",
    "# Raise counts to the 0.75 power (as in the original word2vec paper).\n",
    "freqs = freqs ** 0.75\n",
    "neg_sampling_prob = freqs / freqs.sum()\n",
    "# Convert to a torch tensor for sampling.\n",
    "neg_sampling_prob_tensor = torch.tensor(neg_sampling_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54bbb4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2  # context window on each side\n",
    "\n",
    "def generate_skipgram_pairs(token_list, vocab, window_size):\n",
    "    \"\"\"Generate (target, context) pairs from a tokenized sentence.\"\"\"\n",
    "    pairs = []\n",
    "    indices = [vocab[token] for token in token_list if token in vocab]\n",
    "    for i, target in enumerate(indices):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(indices), i + window_size + 1)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                pairs.append((target, indices[j]))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dca3a203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total positive pairs: 2318548\n"
     ]
    }
   ],
   "source": [
    "all_pairs = []\n",
    "for tokens in df['tokens']:\n",
    "    pairs = generate_skipgram_pairs(tokens, vocab, window_size)\n",
    "    all_pairs.extend(pairs)\n",
    "\n",
    "print(\"Total positive pairs:\", len(all_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf183e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs, neg_sampling_prob_tensor, num_negative):\n",
    "        self.pairs = pairs\n",
    "        self.num_negative = num_negative\n",
    "        self.neg_sampling_prob_tensor = neg_sampling_prob_tensor\n",
    "        self.vocab_size = len(neg_sampling_prob_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        target, context = self.pairs[idx]\n",
    "        # Vectorized negative sampling using torch.multinomial.\n",
    "        negatives = torch.multinomial(\n",
    "            self.neg_sampling_prob_tensor, self.num_negative, replacement=True\n",
    "        )\n",
    "        return (torch.tensor(target, dtype=torch.long),\n",
    "                torch.tensor(context, dtype=torch.long),\n",
    "                negatives)\n",
    "\n",
    "num_negative = 5\n",
    "dataset = SkipGramDataset(all_pairs, neg_sampling_prob_tensor, num_negative)\n",
    "trainloader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f201265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 6. Define the Skip-Gram Model with Negative Sampling\n",
    "# -------------------------\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, target, context, negatives):\n",
    "        # Get embeddings for target, positive context, and negatives.\n",
    "        v_target = self.embeddings(target)        # shape: [batch_size, embedding_dim]\n",
    "        v_context = self.out_embeddings(context)    # shape: [batch_size, embedding_dim]\n",
    "        v_negatives = self.out_embeddings(negatives)  # shape: [batch_size, num_negative, embedding_dim]\n",
    "        \n",
    "        # Positive score: dot product between target and context embeddings.\n",
    "        pos_score = torch.sum(v_target * v_context, dim=1)  # shape: [batch_size]\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-10)\n",
    "        \n",
    "        # Negative score: dot product between target and negative embeddings.\n",
    "        # Using batch matrix multiplication.\n",
    "        neg_score = torch.bmm(v_negatives, v_target.unsqueeze(2)).squeeze(2)  # shape: [batch_size, num_negative]\n",
    "        neg_loss = -torch.sum(torch.log(1 - torch.sigmoid(neg_score) + 1e-10), dim=1)\n",
    "        \n",
    "        loss = torch.mean(pos_loss + neg_loss)\n",
    "        return loss\n",
    "\n",
    "embedding_dim = 256\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "392fb411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Avg Loss: 12.5794\n",
      "Epoch 2, Avg Loss: 4.0337\n",
      "Epoch 3, Avg Loss: 2.9767\n",
      "Epoch 4, Avg Loss: 2.6732\n",
      "Epoch 5, Avg Loss: 2.5252\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for target, context, negatives in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(target, context, negatives)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * target.size(0)\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3762ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "torch.save(model.embeddings.weight, 'skipgram_embeddings.pt')\n",
    "\n",
    "with open('vocab.json', 'w') as f:\n",
    "    json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62703b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIL721",
   "language": "python",
   "name": "ail721"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
